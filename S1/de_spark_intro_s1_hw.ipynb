{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7118e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Домашнее задание:\n",
    "# Условие: Найти самую длинную последовательность упорядоченных чисел в RDD и вывести её"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67de031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81316d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51bb668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "edace5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, (30, None, 0)),\n",
      " (1, (150, 30, 0)),\n",
      " (2, (180, 150, 0)),\n",
      " (3, (-10, 180, 1)),\n",
      " (4, (56, -10, 0)),\n",
      " (5, (34, 56, 1)),\n",
      " (6, (24, 34, 1)),\n",
      " (7, (45, 24, 0)),\n",
      " (8, (67, 45, 0)),\n",
      " (9, (123, 67, 0)),\n",
      " (10, (150, 123, 0)),\n",
      " (11, (44, 150, 1)),\n",
      " (12, (32, 44, 1)),\n",
      " (13, (66, 32, 0)),\n",
      " (14, (77, 66, 0)),\n",
      " (15, (88, 77, 0))]\n",
      "[(0, 30),\n",
      " (0, 150),\n",
      " (0, 180),\n",
      " (1, -10),\n",
      " (1, 56),\n",
      " (2, 34),\n",
      " (3, 24),\n",
      " (3, 45),\n",
      " (3, 67),\n",
      " (3, 123),\n",
      " (3, 150),\n",
      " (4, 44),\n",
      " (5, 32),\n",
      " (5, 66),\n",
      " (5, 77),\n",
      " (5, 88)]\n",
      "Самая длинная последовательность : (24, 45, 67, 123, 150)\n"
     ]
    }
   ],
   "source": [
    "if 'sc' in locals():\n",
    "    sc.stop()\n",
    "sc = SparkContext(\"local\", \"Ordered_sequence\")\n",
    "data = [30, 150, 180, -10, 56, 34, 24, 45, 67, 123, 150, 44, 32, 66, 77, 88]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd_i = rdd.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "rdd_j = rdd_i.leftOuterJoin(rdd_i.map(lambda x: (x[0] + 1, x[1]))).sortByKey().mapValues(lambda v: (v[0], v[1], 0 if (v[1] is None) or (v[0] > v[1]) else 1))\n",
    "pprint(rdd_j.collect())\n",
    "# Часть когда для каждой последовательности присваивается отдельный индекс, я не смог разобраться, как это сделать в Spark\n",
    "i = 0\n",
    "data = []\n",
    "for row in rdd_j.collect():\n",
    "    i += row[1][2]\n",
    "    data.append((i, row[1][0]))\n",
    "pprint(data)\n",
    "# Далее снова все делаем на Spark\n",
    "rdd = sc.parallelize(data)\n",
    "# посчитаем сколько нужно партиций. Хардкод конечно, но не удалось найти, как разделить RDD на партиции без указания их числа, только по значению ключа\n",
    "num_partitions = rdd.keys().distinct().count()\n",
    "# разделим на RDD на партиции и выделим их в явном виде\n",
    "rdd_part = rdd.partitionBy(num_partitions ,lambda k: k).glom()\n",
    "longest_seq = rdd_part.map(lambda x: tuple(el[1] for el in x)).max(key=lambda x: len(x))\n",
    "print(f'Самая длинная последовательность : {longest_seq}')\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1479a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
